{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='14px'>\n",
    "PyTorch: 01 Review Deeplearning\n",
    "</font>\n",
    "<br>\n",
    "\n",
    "[©2020 AI在弦上](https://www.dataml.cn) <a href=\"https://github.com/dataml-cn/playground\"><img src=\"../99_resources/11_img/git.png\" style=\"width:18px; display:inline;margin-bottom:2px\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**内容简介**\n",
    "\n",
    "1. 简单介绍, 机器学习相关定义和一般套路\n",
    "2. 对常见深度学习的网络结构进行梳理, 然后对激活函数做了细致探讨\n",
    "\n",
    "**预期目标**\n",
    "\n",
    "1. ★ 掌握机器学习的基本套路, 在实际项目中可以按照一般套路分析问题\n",
    "2. ★ 掌握基本的网络结构, (多层)感知机、CNN、RNN、Seq2Seq等\n",
    "3. ★ 根据激活函数自身特点, 在项目中灵活选择合适的激活函数\n",
    "\n",
    "\n",
    "## 时间成本\n",
    "- 阅读 10 分钟\n",
    "- 理解 20 分钟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义\n",
    "\n",
    "- improve their **performance** P\n",
    "- at some **task** T\n",
    "- with **experience** E\n",
    "\n",
    "\n",
    "## 套路\n",
    "\n",
    "这里以分类问题的一般形式为例, 简要说明\n",
    "\n",
    "$classify(\\mathbf{x}, \\mathbf{w}) = \\arg\\max\\limits{y}\\ score(\\mathbf{x}, y, \\mathbf{w})$\n",
    "\n",
    "\n",
    "1. Modeling, define score function\n",
    "\n",
    "  - 假设空间, 在什么范围内学习, 选择哪一类模型\n",
    "  - 度量准则, 模型最优准则\n",
    "<br>\n",
    "<br>\n",
    "2. **Learning**, choose $w$\n",
    "  \n",
    "  通常将学习过程, 转化为凸优优化问题\n",
    "\n",
    "  - 损失函数, 度量准则直观, 但通常关于参数非可微, 或者非凸函数\n",
    "  - 最优化算法, 常见SGD、ADAM等 \n",
    "<br>\n",
    "<br>\n",
    "3. Inference, solve argmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 深度学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 网络结构\n",
    "\n",
    "- Perceptron\n",
    "\n",
    "  ![Perceptron](https://pythonmachinelearning.pro/wp-content/uploads/2017/09/Single-Perceptron.png)\n",
    "\n",
    "  (图片来源: Steven Vuong, [How Machines ‘Learn’ — AI: Explainer and Examples](https://medium.com/analytics-vidhya/how-machines-learn-ai-explainer-and-examples-daa71a472716))\n",
    "\n",
    "\n",
    "- Multilayer Perceptron\n",
    "\n",
    "  ![Multilayer](https://dzone.com/storage/temp/3627042-mlp-network.png)\n",
    "\n",
    "  (图片来源: Vojtech Pavlovsky, [Introduction To Artificial Neural Networks – Vojtech Pavlovsky](https://www.vaetas.cz/posts/introduction-artificial-neural-networks/))\n",
    "\n",
    "- Converlutional Neural Network\n",
    "\n",
    "  - input\n",
    "  - converlutional layer\n",
    "  - max pooling layer\n",
    "  - ...\n",
    "  - full connected layer\n",
    "  - output layer\n",
    "\n",
    "  ![Converlutional Neural Network](https://camo.githubusercontent.com/269e3903f62eb2c4d13ac4c9ab979510010f8968/68747470733a2f2f7261772e6769746875622e636f6d2f746176677265656e2f6c616e647573655f636c617373696669636174696f6e2f6d61737465722f66696c652f636e6e2e706e673f7261773d74727565)\n",
    "  (图片来源: tavgreen, [Convolutional Neural Network and Multi Layer Perceptron in Pytorch](https://github.com/tavgreen/cnn-and-dnn))\n",
    "\n",
    "\n",
    "- Recurrent Neural Network\n",
    "\n",
    "  ![Recurrent Neural Network](https://www.researchgate.net/profile/Tao_Jiang10/publication/320441691/figure/fig4/AS:631629156454420@1527603534434/A-RNN-architecture-that-considers-the-extracted-features-in-the-former-state-as-the-one.png)\n",
    "  (图片来源: Tao Jiang, [A-RNN-architecture-that-considers-the-extracted-features-in-the-former-state-as-the-one](https://www.researchgate.net/figure/A-RNN-architecture-that-considers-the-extracted-features-in-the-former-state-as-the-one_fig4_320441691))\n",
    "\n",
    "\n",
    "- Sequence 2 Sequence\n",
    "  \n",
    "  ![Sequence 2 Sequence](https://cdn-images-1.medium.com/max/1600/1*sO-SP58T4brE9EHazHSeGA.png)\n",
    "  (图片来源: Sachin Abeywardana, [Sequence to sequence tutorial – Towards Data Science](https://towardsdatascience.com/sequence-to-sequence-tutorial-4fde3ee798d8))\n",
    "  \n",
    "  ![Sequence 2 Sequence 2](https://wanasit.github.io/static/attention-seq2seq-bottlenect.png)\n",
    "  (图片来源: wanasit,[Attention-based Sequence-to-Sequence in Keras](https://wanasit.github.io/attention-based-sequence-to-sequence-in-keras.html))\n",
    "  \n",
    "\n",
    "- seq2seq Attation\n",
    "  \n",
    "  ![seq2seq Attation](https://i1.wp.com/syncedreview.com/wp-content/uploads/2017/09/image-125.png?resize=666%2C494&ssl=1)\n",
    "\n",
    "  (图片来源: Synced,[A Brief Overview of Attention Mechanism](https://syncedreview.com/2017/09/25/a-brief-overview-of-attention-mechanism/))\n",
    "\n",
    "## 激活函数\n",
    "\n",
    "\n",
    "### 常见激活函数\n",
    "\n",
    "  ![activation function emotion](https://i0.wp.com/sefiks.com/wp-content/uploads/2020/02/sample-activation-functions-square.png?resize=1024%2C806&ssl=1)\n",
    "  \n",
    "  (图片来源: Sefik Ilkin Serengil, [Dance Moves of Deep Learning Activation Functions](https://sefiks.com/2020/02/02/dance-moves-of-deep-learning-activation-functions/)\n",
    "\n",
    "  ![activation function2](https://www.researchgate.net/profile/Hoon_Chung2/publication/309775740/figure/fig1/AS:538049215381504@1505292337270/The-most-common-nonlinear-activation-functions.png)\n",
    "  \n",
    "  (图片来源: Hoon Chung, [Deep neural network using trainable activation functions](https://www.researchgate.net/profile/Hoon_Chung2/publication/309775740_Deep_neural_network_using_trainable_activation_functions/links/59b8efe4a6fdcc68722e33a9/Deep-neural-network-using-trainable-activation-functions.pdf?origin=publication_detail)\n",
    "\n",
    "\n",
    "### 激活函数性质\n",
    "\n",
    "- 计算简单\n",
    "\n",
    "  - 激活函数在神经网络的前向过程中, 计算次数与神经元个数成正比\n",
    "\n",
    "\n",
    "- 非线性\n",
    "\n",
    "  - 否则多层神经网络将退化成单层线性网络\n",
    "\n",
    "\n",
    "- 几乎处处可微\n",
    "\n",
    "  - 梯度可计算\n",
    "\n",
    "\n",
    "- 非饱和性\n",
    "\n",
    "  - 饱和指在某些区间接近于零(梯度消失), 参数无法继续更新\n",
    "  - 可以用于构建更深的网络\n",
    "\n",
    "\n",
    "- 单调性\n",
    "\n",
    "  - 导数符号不变, 梯度方向不经常改变, 让训练更容易\n",
    "  - 单层网络, 保凸性\n",
    "\n",
    "\n",
    "- 接近恒等变换\n",
    "\n",
    "  - 网络稳定, 输出值的幅度不会随深度的增加而发生显著的变化\n",
    "  - 梯度更容易回传\n",
    "\n",
    "\n",
    "- 输出范围有限\n",
    "\n",
    "  - 范围有限, 基于梯度的优化方法更稳定\n",
    "  - 输出无限, 模型的训练相对会更加高效\n",
    "\n",
    "- 参数少\n",
    "\n",
    "  - 大部分激活函数没有参数\n",
    "  \n",
    "  \n",
    "- 归一化\n",
    "  \n",
    "  - 使分布自动归一化0均值, 单位方差的分布\n",
    "  - 训练稳定性\n",
    "\n",
    "\n",
    "### 几种激活函数优缺点对比\n",
    "\n",
    "| 函数 | 物理含义 | 非饱和性 | 非偏移 |计算简单| 应用场景 | 备注 |\n",
    "| :---- | :---- | :---- | :---- | :---- | :---- |\n",
    "| sigmoid | 1. 物理意义更接近神经元<br>2. 输出区间(0,1), 可以被表示概率<br>3. 或者用于输入归一化 | 两侧区间饱和<br>5层神经网络就会产生梯度消失现象 | 下一层网络的输入均值大于0 | 1. sigmoid 自身包含除法<br>2. 导数$\\phi(x) = \\phi(x)(1 - \\phi(x))$ | 1. 概率模型<br>2. RNN中 gate||\n",
    "| tanh | -- | 饱和区域较sigmoid小 | 非偏移 | 与sigmoid类似 | 1. RNN中 gate  | |\n",
    "| ReLu | -- | 输入落入硬饱和区x<0, 权重无法更新(神经元死亡) | 偏移 |1. 自身简单<br>2. 导数, 大于0为1, 其它0 | | 避免过拟合 |\n",
    "\n",
    "\n",
    "**参考**\n",
    "\n",
    "1. Hengkai Guo, 激活函数性质(知乎回答), https://www.zhihu.com/question/67366051\n",
    "2. yyHaker, 常见的激活函数（activation function）总结, https://zhuanlan.zhihu.com/p/70810466"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}